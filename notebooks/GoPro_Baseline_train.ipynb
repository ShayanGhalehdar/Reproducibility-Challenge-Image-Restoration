{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "# Print GPU details\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgX94tYHg8Qp",
        "outputId": "d876499c-d4a8-43b2-be0e-418fc451d29d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "train_input_images_folder = '/content/drive/My Drive/train/input/'\n",
        "train_target_images_folder = '/content/drive/My Drive/train/target/'\n",
        "\n",
        "train_input_patches_folder = \"train_input_patches\"\n",
        "os.makedirs(train_input_patches_folder, exist_ok=True)\n",
        "\n",
        "train_target_patches_folder = \"train_target_patches\"\n",
        "os.makedirs(train_target_patches_folder, exist_ok=True)\n",
        "\n",
        "test_input_lmdb_folder = '/content/drive/My Drive/test/input.lmdb/'\n",
        "test_target_lmdb_folder = '/content/drive/My Drive/test/target.lmdb/'\n",
        "\n",
        "test_input_images_folder = \"test_input_images\"\n",
        "os.makedirs(test_input_images_folder, exist_ok=True)\n",
        "\n",
        "test_target_images_folder = \"test_target_images\"\n",
        "os.makedirs(test_target_images_folder, exist_ok=True)\n",
        "\n",
        "test_input_patches_folder = \"test_input_patches\"\n",
        "os.makedirs(test_input_patches_folder, exist_ok=True)\n",
        "\n",
        "test_target_patches_folder = \"test_target_patches\"\n",
        "os.makedirs(test_target_patches_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "10MT5VSU7cSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58206c56-a227-47a1-c19b-a46ea6e87ee9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# patcher function\n",
        "\n",
        "def compute_stride(image_size, patch_size):\n",
        "    \"\"\"\n",
        "    Compute the optimal stride to ensure full coverage while maximizing uniform patches.\n",
        "    \"\"\"\n",
        "    num_patches = (image_size + patch_size - 1) // patch_size  # Equivalent to math.ceil\n",
        "    return (image_size - patch_size) // (num_patches - 1) if num_patches > 1 else 1\n",
        "\n",
        "def extract_patches(image, patch_size=256):\n",
        "    \"\"\"\n",
        "    Extracts patches ensuring full coverage.\n",
        "    \"\"\"\n",
        "    _, h, w = image.shape\n",
        "    stride_h = compute_stride(h, patch_size)\n",
        "    stride_w = compute_stride(w, patch_size)\n",
        "\n",
        "    # Generate patch indices efficiently\n",
        "    i_vals = torch.arange(0, h - patch_size + 1, stride_h)\n",
        "    j_vals = torch.arange(0, w - patch_size + 1, stride_w)\n",
        "\n",
        "    # Compute number of patches\n",
        "    num_patches_h = i_vals.shape[0]\n",
        "    num_patches_w = j_vals.shape[0]\n",
        "\n",
        "    # Preallocate tensor for patches\n",
        "    patches = torch.empty((num_patches_h * num_patches_w, 3, patch_size, patch_size), dtype=image.dtype, device=image.device)\n",
        "\n",
        "    indices = []\n",
        "    patch_idx = 0\n",
        "    for i in i_vals:\n",
        "        for j in j_vals:\n",
        "            patches[patch_idx] = image[:, i:i+patch_size, j:j+patch_size]\n",
        "            indices.append((i.item(), j.item()))\n",
        "            patch_idx += 1\n",
        "\n",
        "    return patches, indices, stride_h, stride_w"
      ],
      "metadata": {
        "id": "WLCtAJ-87IDi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def save_patches(image_folder, patch_folder, prefix):\n",
        "    \"\"\"\n",
        "    Extracts patches from images in image_folder and saves them in patch_folder.\n",
        "\n",
        "    Args:\n",
        "        image_folder (str): Path to the folder containing images.\n",
        "        patch_folder (str): Path to save patches.\n",
        "        prefix (str): Prefix for naming output patches.\n",
        "    \"\"\"\n",
        "    os.makedirs(patch_folder, exist_ok=True)  # Ensure output folder exists\n",
        "    transform = transforms.ToTensor()\n",
        "    image_counter = 0\n",
        "\n",
        "    # Get sorted list of PNG files\n",
        "    img_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.png')])\n",
        "\n",
        "    for img_file in img_files:\n",
        "        img_path = os.path.join(image_folder, img_file)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        with Image.open(img_path) as img:\n",
        "            img = img.convert(\"RGB\")\n",
        "            image_tensor = transform(img)  # Convert to tensor (C, H, W)\n",
        "\n",
        "        # Extract patches\n",
        "        patches, _, _, _ = extract_patches(image_tensor, patch_size=256)\n",
        "\n",
        "        # Save patches efficiently\n",
        "        for idx, patch in enumerate(patches):\n",
        "            patch_name = f\"{prefix}_{image_counter:05d}_{idx:03d}.png\"\n",
        "            patch_path = os.path.join(patch_folder, patch_name)\n",
        "            transforms.ToPILImage()(patch).save(patch_path)\n",
        "\n",
        "        image_counter += 1\n",
        "\n",
        "        # Print progress every 100 images\n",
        "        if image_counter % 100 == 0:\n",
        "            print(f\"Processed {image_counter} {prefix} images...\")\n",
        "\n",
        "    print(f\"Finished processing {image_counter} {prefix} images.\")\n"
      ],
      "metadata": {
        "id": "Ij4787l4zBuD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_patches(train_input_images_folder, train_input_patches_folder, 'input')\n",
        "save_patches(train_target_images_folder, train_target_patches_folder, 'target')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEC2RaBtkfC0",
        "outputId": "dee178ab-9d94-4976-9fe6-edc35e50f20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 input images...\n",
            "Processed 200 input images...\n",
            "Processed 300 input images...\n",
            "Processed 400 input images...\n",
            "Processed 500 input images...\n",
            "Processed 600 input images...\n",
            "Processed 700 input images...\n",
            "Processed 800 input images...\n",
            "Processed 900 input images...\n",
            "Processed 1000 input images...\n",
            "Processed 1100 input images...\n",
            "Processed 1200 input images...\n",
            "Processed 1300 input images...\n",
            "Processed 1400 input images...\n",
            "Processed 1500 input images...\n",
            "Processed 1600 input images...\n",
            "Processed 1700 input images...\n",
            "Processed 1800 input images...\n",
            "Processed 1900 input images...\n",
            "Processed 2000 input images...\n",
            "Processed 2100 input images...\n",
            "Finished processing 2103 input images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "class PairedPatchDataset(Dataset):\n",
        "    def __init__(self, input_folder, target_folder):\n",
        "        self.input_folder = input_folder\n",
        "        self.target_folder = target_folder\n",
        "\n",
        "        # Use os.scandir() for faster directory listing\n",
        "        input_files = {f.name.replace(\"input_\", \"\").split('.')[0] for f in os.scandir(input_folder) if f.is_file()}\n",
        "        target_files = {f.name.replace(\"target_\", \"\").split('.')[0] for f in os.scandir(target_folder) if f.is_file()}\n",
        "\n",
        "        # Find common filenames\n",
        "        self.image_filenames = sorted(input_files & target_files)\n",
        "\n",
        "        # Precompute full paths for efficiency\n",
        "        self.input_paths = [os.path.join(input_folder, f\"input_{name}.png\") for name in self.image_filenames]\n",
        "        self.target_paths = [os.path.join(target_folder, f\"target_{name}.png\") for name in self.image_filenames]\n",
        "\n",
        "        # Define transformations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  # Convert images to tensors\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images\n",
        "        input_image = Image.open(self.input_paths[idx]).convert(\"RGB\")\n",
        "        target_image = Image.open(self.target_paths[idx]).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations\n",
        "        return self.transform(input_image), self.transform(target_image)"
      ],
      "metadata": {
        "id": "x5-8_eP74xCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BVNR60tftmcf"
      },
      "outputs": [],
      "source": [
        "# baseline block\n",
        "\n",
        "class LayerNorm2d(nn.Module):\n",
        "    def __init__(self, num_channels, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(num_channels, eps=eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert to channels_last format for efficiency\n",
        "        x = x.to(memory_format=torch.channels_last)\n",
        "        return self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=2):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False)  # No bias needed\n",
        "        self.relu = nn.ReLU(inplace=True)  # In-place ReLU saves memory\n",
        "        self.conv2 = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn = self.pool(x)\n",
        "        attn = self.conv1(attn)\n",
        "        attn = self.relu(attn)\n",
        "        attn = self.conv2(attn)\n",
        "        attn = self.sigmoid(attn)\n",
        "        return x * attn\n",
        "\n",
        "class BaselineBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(BaselineBlock, self).__init__()\n",
        "        self.norm1 = LayerNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "        self.dconv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.ca = ChannelAttention(in_channels)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "        self.norm2 = LayerNorm2d(in_channels)\n",
        "        self.conv3 = nn.Conv2d(in_channels, 2 * in_channels, kernel_size=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(2 * in_channels, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.norm1(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.dconv(out)\n",
        "        out = self.gelu(out)\n",
        "        out = self.ca(out)\n",
        "        out = self.conv2(out)\n",
        "        out += residual\n",
        "\n",
        "        residual = out\n",
        "\n",
        "        out = self.norm2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.gelu(out)\n",
        "        out = self.conv4(out)\n",
        "        out += residual\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self, n_channels=3, width=32):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.init_conv = nn.Conv2d(n_channels, width, kernel_size=3, padding=1)\n",
        "\n",
        "        # Encoder (Downsampling path)\n",
        "        self.enc1 = self._make_stage(width, 4)\n",
        "        self.down1 = nn.Conv2d(width, width, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.enc2 = self._make_stage(width, 4)\n",
        "        self.down2 = nn.Conv2d(width, width, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.enc3 = self._make_stage(width, 4)\n",
        "        self.down3 = nn.Conv2d(width, width, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.enc4 = self._make_stage(width, 4)\n",
        "        self.down4 = nn.Conv2d(width, width, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._make_stage(width, 4)\n",
        "\n",
        "        # Decoder (Upsampling path)\n",
        "        self.up4 = self._upsample_layer(width)\n",
        "        self.dec4 = self._make_stage(width, 4)\n",
        "\n",
        "        self.up3 = self._upsample_layer(width)\n",
        "        self.dec3 = self._make_stage(width, 4)\n",
        "\n",
        "        self.up2 = self._upsample_layer(width)\n",
        "        self.dec2 = self._make_stage(width, 4)\n",
        "\n",
        "        self.up1 = self._upsample_layer(width)\n",
        "        self.dec1 = self._make_stage(width, 4)\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_conv = nn.Conv2d(width, n_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def _make_stage(self, channels, num_blocks):\n",
        "        \"\"\"Helper function to create multiple BaselineBlocks.\"\"\"\n",
        "        return nn.Sequential(*[BaselineBlock(channels) for _ in range(num_blocks)])\n",
        "\n",
        "    def _upsample_layer(self, channels):\n",
        "        \"\"\"Upsample using pointwise convolution followed by pixel shuffle.\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(channels, channels * 4, kernel_size=1),\n",
        "            nn.PixelShuffle(2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        x = self.down1(e1)\n",
        "\n",
        "        e2 = self.enc2(x)\n",
        "        x = self.down2(e2)\n",
        "\n",
        "        e3 = self.enc3(x)\n",
        "        x = self.down3(e3)\n",
        "\n",
        "        e4 = self.enc4(x)\n",
        "        x = self.down4(e4)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up4(x) + e4\n",
        "        x = self.dec4(x)\n",
        "\n",
        "        x = self.up3(x) + e3\n",
        "        x = self.dec3(x)\n",
        "\n",
        "        x = self.up2(x) + e2\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        x = self.up1(x) + e1\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        # Final output\n",
        "        x = self.final_conv(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zkyscXw65HnL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# Define PSNR Loss\n",
        "def psnr_loss(pred, target, max_val=1.0):\n",
        "    mse = nn.functional.mse_loss(pred, target)\n",
        "    psnr = 20 * torch.log10(max_val / torch.sqrt(mse))\n",
        "    return -psnr  # Negative because we minimize loss\n",
        "\n",
        "# Dataset (Dummy Dataset Example)\n",
        "class RandomDataset(data.Dataset):\n",
        "    def __init__(self, size=1000, img_size=(3, 256, 256)):\n",
        "        self.size = size\n",
        "        self.img_size = img_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = torch.rand(self.img_size)\n",
        "        target = img.clone()  # Identity mapping as placeholder\n",
        "        return img, target\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 8\n",
        "num_iterations = 10000\n",
        "learning_rate = 1e-3\n",
        "min_lr = 1e-6\n",
        "width = 32  # Model width parameter\n",
        "\n",
        "# Prepare DataLoader\n",
        "dataset = PairedPatchDataset(train_input_patches_folder,train_target_patches_folder)\n",
        "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model Initialization\n",
        "model = BaselineModel(3, width).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.9), weight_decay=0)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_iterations, eta_min=min_lr)\n",
        "\n",
        "# TensorBoard Writer\n",
        "# writer = SummaryWriter()\n",
        "\n",
        "# Training Loop\n",
        "model.train()\n",
        "iteration = 0\n",
        "while iteration < num_iterations:\n",
        "    for imgs, targets in dataloader:\n",
        "        imgs, targets = imgs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = psnr_loss(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Log Metrics\n",
        "        if iteration % 100 == 0:\n",
        "            psnr_value = -loss.item()\n",
        "            #writer.add_scalar('Loss/PSNR', psnr_value, iteration)\n",
        "            print(f\"Iteration {iteration}: PSNR = {psnr_value:.2f} dB\")\n",
        "\n",
        "        iteration += 1\n",
        "        if iteration >= num_iterations:\n",
        "            break\n",
        "\n",
        "#writer.close()\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "am8haw0Zh3sA",
        "outputId": "bbbaaa0c-0138-4e86-c923-6a2c7b454558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: PSNR = 2.17 dB\n",
            "Iteration 5: PSNR = 12.27 dB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5d3b59e43b0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsnr_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lmdb\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def extract_images_from_lmdb(lmdb_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Extracts PNG images from an LMDB database and saves them to the specified folder.\n",
        "\n",
        "    Args:\n",
        "        lmdb_folder (str): Path to the LMDB folder containing data.mdb and lock.mdb.\n",
        "        output_folder (str): Path to the folder where extracted images will be saved.\n",
        "    \"\"\"\n",
        "    # Open LMDB environment\n",
        "    env = lmdb.open(lmdb_folder, readonly=True, lock=False)\n",
        "\n",
        "    with env.begin() as txn:\n",
        "        cursor = txn.cursor()\n",
        "        for key, value in cursor:\n",
        "            # Decode the key (filename)\n",
        "            filename = key.decode(\"utf-8\")\n",
        "\n",
        "            # Ensure filename has .png extension\n",
        "            if not filename.endswith(\".png\"):\n",
        "                filename += \".png\"\n",
        "\n",
        "            # Convert value (image data) to numpy array\n",
        "            img_array = np.frombuffer(value, dtype=np.uint8)\n",
        "            img = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)  # Read as an image\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Warning: Could not decode image for key {filename}, skipping...\")\n",
        "                continue  # Skip if decoding fails\n",
        "\n",
        "            # Ensure output folder exists\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "            # Save the image\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, img)\n",
        "            print(f\"Saved {output_path}\")\n",
        "\n",
        "    print(\"Extraction complete!\")\n"
      ],
      "metadata": {
        "id": "LhbKo60jk4Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_images_from_lmdb(test_input_lmdb_folder, test_input_images_folder)\n",
        "extract_images_from_lmdb(test_target_lmdb_folder, test_target_images_folder)"
      ],
      "metadata": {
        "id": "2DPoym7A2eqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_patches(test_input_images_folder, test_input_patches_folder, 'input')\n",
        "save_patches(test_target_images_folder, test_target_patches_folder, 'target')"
      ],
      "metadata": {
        "id": "Ga5uk91mqEL-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
